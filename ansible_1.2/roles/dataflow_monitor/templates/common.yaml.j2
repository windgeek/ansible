kafka:
  producerMap:
    "bootstrap.servers":"{{ dataflow.kafka }}"
    "acks":"all"
    "batch.size":102400
    "linger.ms":10
    "client.id":"spark_p"
    "buffer.memory":33554432
    "compression.type":"lz4"
    "max.request.size":1048576
    "retries":0
    "key.serializer":"org.apache.kafka.common.serialization.StringSerializer"
    "value.serializer":"org.apache.kafka.common.serialization.StringSerializer"
  consumerMap:
    "bootstrap.servers":"{{ dataflow.kafka }}"
    "client.id":"spark_c"
    "auto.offset.reset":"latest"
    "max.poll.records":300
    "session.timeout.ms":60000
    "heartbeat.interval.ms":10000
    "max.partition.fetch.bytes":5242880
    "request.timeout.ms":90000
    "fetch.max.wait.ms":1000
spark:
  intervalTime: 5
clickHouse:
  insertBlukSize: 30000
  ipPort: "{{ dataflow.common_clickhouse }}:8123"
  database: "k19_ods"
  userName: "writer"
  password: "k18"
mysql:
  driver: "com.mysql.cj.jdbc.Driver"
  url: "jdbc:mysql://{{ dataflow.mysql_host}}:3306/geo_ip?serverTimezone=CTT&useUnicode=true&characterEncoding=utf-8&allowMultiQueries=true"
  userName: "{{ dataflow.mysql_user }}"
  password: "{{ dataflow.mysql_password }}"
  tableName: "ip_location"
monitor:
  offSwitch: false
  errorMessageTopic: "K19-SPARK-EXCEPTION-LOG"
  exceptionSize: 10000
  flushSwitch: false
region: "TSE"
logLevel: "INFO"